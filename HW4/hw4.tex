\documentclass{homeworg}

\title{CSEP 521, Winter 2021: Homework 4}
\author{Krishan Subudhi (ksubudhi@uw.edu) - 2040900}
\usepackage[table]{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{pythonhighlight}
\usepackage{enumitem}
\usepackage{array}

\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{placeins}

\let\Oldsubsection\subsection
\renewcommand{\subsection}{\FloatBarrier\Oldsubsection}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

\begin{document}

\maketitle

\exercise
\emph{Q: Show that the number 1048579 = 220 + 3 is not prime without factoring the number}

Fermat's theorm : 
\[a^{p-1} \mod p = 1,  \text{if $p$ is prime.}\]

$p = 1048579 = 2^{20} + 3$

Let's take $a = 2$. Now if we can prove $2^{2^{20}+2} \mod 1048579$ is not 1, then p is not a prime.

\begin{align*}
2^{2^3} \mod 1048579 &= (2^{2^2} \mod 1048579 \ast 2^{2^2} \mod 1048579) \mod 1048579 &= 256\\
2^{2^4} \mod 1048579 &= (2^{2^3} \mod 1048579 \ast 2^{2^3} \mod 1048579) \mod 1048579 &= 65536\\
2^{2^5} \mod 1048579 &= (2^{2^4} \mod 1048579 \ast 2^{2^4} \mod 1048579) \mod 1048579 &= 1036291\\
2^{2^6} \mod 1048579 &= (2^{2^5} \mod 1048579 \ast 2^{2^5} \mod 1048579) \mod 1048579 &= 1048147\\
2^{2^7} \mod 1048579 &= (2^{2^6} \mod 1048579 \ast 2^{2^6} \mod 1048579) \mod 1048579 &= 186624\\
\texttt{...}\\
\texttt{...}\\
2^{2^{19}} \mod 1048579 &= (2^{2^{18}} \mod 1048579 \ast 2^{2^{18}} \mod 1048579) \mod 1048579 &= 870510\\
2^{2^{20}} \mod 1048579 &= (2^{2^{19}} \mod 1048579 \ast 2^{2^{19}} \mod 1048579) \mod 1048579 &= 588380
\end{align*}

Hence ,

\[
2^{2^{20}+2} \mod 1048579 = ( {2^{2^{20}} \mod 1048579 \ast 2^{2} \mod 1048579} ) \mod 1048579 = 256362
\]

Since $256362 \ne 1$, the number 1048579 is not a prime.

\newpage
\exercise

A hash table of size m is used to store n items, with $n \le m/2$, so the load factor is at most $\frac{1}{2}$

Open addressing is used for collision resolution.

\emph{a) Assuming uniform hashing, show that for $i =1, 2,. .., n$, the probability that the $i$-th insertion requires strictly more than k probes is at most $2^{-k}$}

P ($i$-th insertion requires strictly more than k probes is at most $2^{-k}$)

= P ( first $k$ hashes in $i$-th iteration end up in collision)
\begin{align*}
P &=\left(\frac{i-1}{m}\right)^k\\
&\le \left(\frac{n}{m}\right)^k\\
&\le \left(\frac{m/2}{m}\right)^k\\
&\le 2^{-k}
\end{align*}

\emph{b) Show that for $i =1, 2,. .., n$, the probability that the $i$-th insertion requires more than $2 \log n$ probes is at most $1/n^2$.}
We previously showed in (a) that ,

$$P (i-\text{th insertion requires strictly more than k probes}) \le 2^{-k}$$
Hence for $k = 2 \log n$,
\begin{align*}
    P (i-\text{th insertion requires strictly more than $2 \log n $ probes})&\le 2^{-2 \log n}\\
    &\le \frac{1}{n^2} 
\end{align*}

Hence, the probability that the $i$-th insertion requires more than $2 \log n$ probes is at most $1/n^2$


\textbf{c,d Definitions}:

$X_i$ = the number of probes required by the $i$-th insertion
As per (b), $P({X_i > 2 \log n}) \le 1/n^2$ 

$X = max_{1\le i\le n} X_i$ = maximum number of probes required by any of the $n$ insertions

\emph{c) Show that $Pr({X> 2 \log n}) \le 1/n$}

\begin{align*}
    P({X> 2 \log n}) &= P((X_1> 2 \log n) \cup (X_2> 2 \log n) \cup... \cup (X_n> 2 \log n)) \\
    &\le P((X_1> 2 \log n) + (X_2> 2 \log n) +... + (X_n> 2 \log n)) \\
    &\le n \ast  \frac{1}{n^2} \\
    &\le \frac{1}{n} \numberthis \label{eqn_c}
\end{align*}
Proved

\emph{d) Show that the expected length of the longest probe sequence is $E[X]= O(\log n)$}

\begin{align*}
    E(X) &= \sum_{k=1}^n k \ P(X=k)\\
    &= \sum_{k=1}^{2 \log n} k \ P(X=k) + \sum_{n > 2 \log n }^n k \ P(X=k)\\
    &\le 2 \log n \sum_{k=1}^{2\log n} P(X=k) + n \sum_{k > 2\log n}^n P(X=k) \numberthis \label{eqn_d1}
\end{align*}
As per \ref{eqn_c}, $Pr({X> 2 \log n}) \le 1/n$.

\[
    \implies \sum_{k > 2\log n}^n P(X=k) = Pr({X> 2 \log n}) \le \frac{1}{n}
\]

Also,
\[
    \sum_{k=1}^{2\log n} P(X=k) = Pr({X\le 2 \log n}) \le 1
\]
Hence equation \ref{eqn_d1} can be written as
\begin{align*}
    E(X) &\le 2 \log n \ast 1 +n \ast \frac{1}{n}\\
    &\le 2\log n +1 \numberthis\label{eqn_d2}
\end{align*}

Hence as per \ref{eqn_d2} ,     $E(X) = O(\log n)$.

\newpage
\exercise
We have a bloom filter that uses a table of size $n$ with $k$ hash functions.

We are asked to adapt your Bloom filter to a setting where the table is of size $n/2$

Let's assume that $n$ is a power of $2$.
\[
    n = 2^x
\]

To reduce the size of exiting boom filter but retain current information, we can \begin{itemize}
    \item $OR$ the two halves. The first half will be replaced with the bits from the $OR$ operation. The 2nd half will be discarded.
    \item The hash functions needs to map bits in 2nd half to the corresponding bits in the first half. 
    
    Since $n=2^x$ , the hash function must be using $x$ bits for finding bit location in the bloom filter. Now if we mask the most significant bit, from the hash function output, the remaining $x-1$ bits will map to the first half of the bloom filter. For example if n =8, and hash function output was 7(111) before, now it will map to the $3^{rd}$ bit .
    \item The $OR$ operation + masking will avoid losing existing data.
\end{itemize}

Let's say the bloom filter had $m$ entries before. 

Previously before shrinking, 
\begin{align*}
    P(\text{a particular bit is 0}) &= \left(1-\frac{1}{n}\right)^{km} = e^{-km/n}\\
    P(\text{fasle positive}) &= (1-P(\text{a particular bit is 0}))^k\\
    &= (1-e^{-km/n})^k = (1-p)^k\numberthis\label{eqn_3_before}
\end{align*}
Where $p = e^{-km/n}$.

After shrinking, 
\begin{align*}
    P(\text{a particular bit i is 0}) &= P(\text{a particular bit i is 0}) \ast P(\text{a particular bit n/2+i was 0})  \\
    &= e^{-km/n} \ast e^{-km/n} = p^2\\
    P(\text{fasle positive}) &= (1-P(\text{a particular bit i is 0}))^k\\
    &= (1-p^2)^k\numberthis\label{eqn_3_after}
\end{align*}

Hence as per \ref{eqn_3_before}, and \ref{eqn_3_after},
False positive probability will increase by $(1-p^2)^k/ (1-p)^k = (1+p)^k$ times. 
\newpage
\exercise
'''
\subsection{Algorithm}
Two choice hashing uses two hash function and assigns items to the least used bucket. 

\subsection{Assumptions}
For this problem, you will assume a chaining implementation of hashing, since you are counting the number of elements assigned to buckets (but you donâ€™t need to implement chaining, since you only need the counts.


You may assume that your hash functions are perfectly random. 

For each experiment you run, you should hash n items into n cells. 

\subsection{Metric}
The statistic of most interest is the maximum number of items assigned to a cell, 

but computing the standard deviation is also of interest.

\subsection{Description}
Here I have implemented $k$ choice hashing. $n$ items are hashed using $k$ hash functions(simulated). Item will be assigned to one of the buckets selected by the hash functions. The bucket with the lowest count will be chosen as the destination for each element.

\subsection{Result}
\begin{table}[h]
    \centering
    \begin{tabular}{llrrrr}
    \toprule
            &   &  max\_items &  max\_items/log logn &  max\_items/(log n/ log log n) &  Standard Deviation \\
    n & k &            &                     &                               &                     \\
    \midrule
    1000    & 1 &          5 &            2.587128 &                      1.398895 &            1.027619 \\
            & 2 &          3 &            1.552277 &                      0.839337 &            0.711337 \\
            & 3 &          2 &            1.034851 &                      0.559558 &            0.589915 \\
    10000   & 1 &          8 &            3.603073 &                      1.928551 &            1.001798 \\
            & 2 &          3 &            1.351152 &                      0.723207 &            0.705833 \\
            & 3 &          3 &            1.351152 &                      0.723207 &            0.595483 \\
    100000  & 1 &          8 &            3.274032 &                      1.697897 &            1.001868 \\
            & 2 &          4 &            1.637016 &                      0.848949 &            0.702382 \\
            & 3 &          3 &            1.227762 &                      0.636711 &            0.594559 \\
    1000000 & 1 &          9 &            3.427537 &                      1.710550 &            1.000499 \\
            & 2 &          4 &            1.523350 &                      0.760245 &            0.704036 \\
            & 3 &          3 &            1.142512 &                      0.570183 &            0.595970 \\
    \bottomrule
    \end{tabular}

    \caption{$k$ choice hashing for different values of $n$}
    \label{tab:q4_result}
\end{table}

For k = 1, max\_items grow at the rate $O(logn/loglogn)$. For k>1, max\_items grows at the rate $O(loglogn)$ and    onstant factor decreases with increase in $k$ . Standard deviation is also lower for higher $k$ which means the distribution of items in buckets are more even with higher $k$.

\begin{figure}
    \centering
    \includegraphics{images/range_k.png}
    \caption{k-choice hashing performance as k ranges from 1 to n}
    \label{fig:my_label}
\end{figure}
\begin{python}
import random
import numpy as np
import logging
for handler in logging.root.handlers[:]:
    logging.root.removeHandler(handler)
class KchoiceHashing():
    def __init__(self, n, k):
        self.n = n
        self.k = k
        # Buckets only store counts in this simulation.
        self.buckets = [0] * self.n #nothing is assigned to the buckets yet
        self.logger = logging.getLogger(name = __name__)
        # random hash functions with different seeds.
        # Each hash function is a random number generator.
        self.h = [random.Random(i) for i in range(k)]

    def assign_item(self, item = None):
        '''Assigns a new item to a bucket.
        For this simulation, we don't need to assign actual items.
        We will just increment item count in the assigned bucket 
        '''
        # Compute hash functions and shortlist buckets
        selected_buckets = [h.randint(0, n-1) for h in self.h]
        current_items = np.array([self.buckets[b] for b in selected_buckets])
        # select bucket with least amount of items in it
        min_bucket = selected_buckets[np.argmin(current_items)]

        self.logger.debug(f'selected bucket {min_bucket} from {selected_buckets}')

        # assign item to that bucket
        self.buckets[min_bucket] += 1

        self.logger.debug(self.buckets)

    def assign_n_items(self):
        for i in range(n):
            self.logger.debug(f'iteration {i}')
            self.assign_item(i)
    
    def max_items(self):
        return max(self.buckets)

    def std(self):
        return np.array(self.buckets).std()

import pandas as pd
if __name__ == '__main__':

    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(name = __name__)
    series = []
    for n in [1_000,10_000,100_000,1_000_000]:
        for k in [1,2,3]:
            hashing = KchoiceHashing(n,k)
            hashing.assign_n_items()
            max_items = hashing.max_items()
            ratio = max_items/np.log(np.log(n))
            ratio2 = max_items/(np.log(n)/np.log(np.log(n)))
            s = {
                'n':n,
                'k':k,
                'max_items':max_items,
                'max_items/log logn': ratio,
                'max_items/(log n/ log log n)': ratio2,
                'standard Deviation':hashing.std()
            }
            series.append(s) 
            logger.info(f'n = {n}, k = {k}, max_items = {max_items}, ratio = {ratio:.3}, std = {hashing.std():.3}')
    df = pd.DataFrame(series)
    df = df.set_index(['n','k'])
    print(df.to_latex())

    df.to_csv('q4_out.csv')
    
    
    n = 1024
    vals = {}
    for k in range(0,int(np.log2(n))+1):
        print(k)
        k = 2**k
        print(k)
        hashing = KchoiceHashing(n,k)
        hashing.assign_n_items()
        max_items = hashing.max_items()
        vals[k]= max_items
    plt.plot(vals.keys(), vals.values())
    plt.xlabel('k')
    plt.ylabel('max_items')
    
    plt.show()
\end{python}


\end{document}